[CRAB]

jobtype = cmssw
# This asks CRAB to choose the server
# Or this lets you choose it.
#server_name = cern

# GRID batch queues
scheduler = glite
#scheduler = condor 

# RAL batch queue
#scheduler = pbs
#use_server = 1

[CMSSW]

### The ParameterSet you want to use
pset=run.py
#pset=MakePatTupleCraig_cfg.py

# Using LHE input from external source ?
#generator = lhe

# -- Data
#pycfg_params=0

#lumi_mask=jsonls.txt.2010A
#datasetpath=/JetMET/Run2010A-Sep17ReReco_v2/RECO

#lumi_mask=jsonls.txt.2010B
#datasetpath=/Jet/Run2010B-PromptReco-v2/RECO 

#runselection=1-9999999

# -- Signal
pycfg_params=u
datasetpath=/HTo2LongLivedTo4F_MH-1000_MFF-350_CTau-350_7TeV-pythia6/Fall11-E7TeV_Ave23_50ns-v1/GEN-SIM-RAW-HLTDEBUG-RECODEBUG

#/HTo2LongLivedTo4F_MH-400_MFF-50_CTau-80_7TeV-pythia6/Fall10-START38_V12-v1/GEN-SIM-RECODEBUG
#/HTo2LongLivedTo4F_MH-400_MFF-20_CTau-40_7TeV-pythia6/Fall10-START38_V12-v1/GEN-SIM-RECODEBUG
#/HTo2LongLivedTo4F_MH-400_MFF-150_CTau-400_7TeV-pythia6/Fall10-START38_V12-v1/GEN-SIM-RECODEBUG
#/HTo2LongLivedTo4F_MH-1000_MFF-50_CTau-40_7TeV-pythia6/Fall10-START38_V12-v1/GEN-SIM-RECODEBUG

# Only needed for private production
#dbs_url = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_02_writer/servlet/DBSServlet

# -- Background
#pycfg_params=2
#datasetpath=/QCD_Pt80/Summer10-START36_V9_S09-v1/GEN-SIM-RECODEBUG
# N.B. Following should not be Spring production. Fix before using.
#datasetpath=/QCD_Pt170/Spring10-START3X_V26_S09-v1/GEN-SIM-RECO 
#datasetpath=/QCD_Pt300/Spring10-START3X_V26_S09-v1/GEN-SIM-RECO 
#datasetpath=/TTbar/Spring10-START3X_V26_S09-v1/GEN-SIM-RECO 

### Splitting parameters

# MC
total_number_of_events=-1
#total_number_of_events=1000
events_per_job = 8000
#number_of_jobs = 40

# For real data (no. lumis approx 6600)
#total_number_of_lumis=25
# Seems that about 100, 50, 25 for data 1,2,3 give right output data size.
#lumis_per_job=300
#lumis_per_job=150
#lumis_per_job=25

# Get anything produced by PoolOutputModule
get_edm_output = 0

### The output files (comma separated list)
output_file = ntupleu.root

[USER]
return_data=0
copy_data=1
# IRT -temporary bug fix for CMSSW 3.3.1
# https://hypernews.cern.ch/HyperNews/CMS/get/crabFeedback/2633/1/1/1.html
#check_user_remote_dir = 0

#additional_input_files = fort.42,fort.62,fort.82,firstTry-single.lhe
#additional_input_files = firstTry-single.lhe

### OUTPUT files Management
##  output back into UI
#return_data = 1

### To use a specific name of UI directory where CRAB will create job to submit (with full path).
### the default directory will be "crab_0_data_time"
#ui_working_dir = /opt/ppd/cms/users/tomalin/MakePatTuple383_HTo2LongLivedTo4F_MH-1000_MFF-150_CTau-100_7TeV_STARTUP_t2glite

### OUTPUT files INTO A SE
#copy_data = 1

### if you want to copy data in a "official CMS site"
### you have to specify the name as written in 
storage_element = T3_US_FNALLPC
### the user_remote_dir will be created under the SE mountpoint
### in the case of publication this directory is not considered
#user_remote_dir = MakePatTuple383_DATA2010B_7TeV_t3
#user_remote_dir = MakePatTuple383_HTo2LongLivedTo4F_MH-1000_MFF-150_CTau-100_7TeV_STARTUP_t2glite
#user_remote_dir = MakePatTuple383_QCD_Pt80_Summer10_7TeV_STARTUP_r1

### if you want to copy your data at CAF
#storage_element = T2_CH_CAF
### the user_remote_dir will be created under the SE mountpoint
### in the case of publication this directory is not considered
#user_remote_dir = name_directory_you_want

### if you want to copy your data to your area in castor at cern
### or in a "not official CMS site" you have to specify the complete name of SE
#storage_element=srm-cms.cern.ch
### this directory is the mountpoin of SE 
#storage_path=/srm/managerv2?SFN=/castor/cern.ch/
### directory or tree of directory under the mounpoint 
#lfn=/user/x/xxx/name_directory_you_want


### To publish produced output in a local istance of DBS set publish_data = 1
#publish_data=1
### Specify the dataset name. The full path will be <primarydataset>/<publish_data_name>/USER
#publish_data_name = MakePatTuple383_DATA2010B_7TeV_t3
#publish_data_name = MakePatTuple383_HTo2LongLivedTo4F_MH-1000_MFF-150_CTau-100_7TeV_STARTUP_t2glite
#publish_data_name = MakePatTuple383_QCD_Pt80_Summer10_7TeV_STARTUP_r1

### Specify the URL of DBS istance where CRAB has to publish the output files
#dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_02_writer/servlet/DBSServlet 

#if server
#thresholdLevel = 90
#eMail = ian.tomalin@cern.ch

[GRID]

## RB/WMS management:
#rb = CERN

##  Black and White Lists management:
## By Storage
#se_black_list = T0,T1,T3
#se_black_list = T0,T1
#se_white_list =

## By ComputingElement
#ce_black_list = gridka.de,ihep.ac.cn,in2p3.fr,ce2.polgrid.pl,kuragua.uniandes.edu.co,lcgce02.jinr.ru;viking.lesc.doc.ic.ac.uk,sinp.msu.ru,ecdf.ed.ac.uk,gridce.iihe.ac.be,colorado.edu,red-gw2.unl.edu,cebo-t3-01.cr.cnaf.infn.it,cream01.iihe.ac.be
#ce_white_list = rl.ac.uk
#ce_white_list = heplnx207.pp.rl.ac.uk,heplnx206.pp.rl.ac.uk

#shallow_retry_count = 2

#[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))

